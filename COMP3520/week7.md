# Memory Management

## Paging

Paging is a memory management scheme that:

- Splits physical memory into fixed-size blocks called frames.
- Splits a process’s logical memory into blocks of the same size called pages.
- Uses a page table to map each page → frame.
  - This way, a process doesn’t need to be stored in one big continuous block
    of memory; its pieces (pages) can be scattered around, yet the program still
    sees them as one continuous memory space.

### Analogy: Paging = Bookshelf and Books

Imagine you have:

- A library (physical memory) with bookshelves (frames), each shelf is the same size.
- A long book (process), but instead of being one giant bound volume, it’s printed
  on individual loose sheets (pages).
- The book is too big to fit on one shelf, so you store its pages across different
  shelves in the library.
- To keep track of where each page went, you have an index card (page table):
  - Page 1 → Shelf 7
  - Page 2 → Shelf 12
  - Page 3 → Shelf 3
  - … and so on.

Now, if you want to read page 2 of the book, you:

1. Look at the index card → find “page 2 is on shelf 12”.
2. Go to shelf 12 and pull out the right sheet.

#### How Addresses Work in This Analogy

- Virtual Address (logical address) = "Book page number + line number on page"
- Physical Address = "Shelf number (frame) + line number on that sheet"
- The page number is like asking “which sheet of the book?”
- The offset is like asking “which line on that sheet?”
- The page table translates page number → shelf number.

## Address Translation

- Virtual (logical) address → Physical address.
- Page/frame size must be a power of 2.
- Split virtual address:
  - High-order bits = Page Number
  - Low-order bits = Offset within the page
- Steps:
  1. Use page number to index page table.
  2. Retrieve frame number from page table entry.
  3. Combine frame number + offset = physical address.
- Page Table Register stores starting address of the page table in memory.

## Example

1. Firefox as a Process
   - When you open Firefox:
     - The OS treats Firefox as a process.
     - That process has a large virtual address space (like its own “private book
       of memory”).
     - Firefox itself doesn’t know or care where the data actually sits in
       physical RAM — it only sees virtual addresses.

2. Pages and Frames
   - Firefox’s memory is split into pages (say, 4KB each).
   - Physical memory (RAM) is split into **frames** of the same size.
   - Firefox’s page table maps its pages → frames in RAM.
   - So if Firefox needs:
     - Page 10 (where YouTube tab’s state is stored),
     - The page table might say: “Page 10 is stored in Frame 247 in RAM.”

3. Switching to YouTube Tab
   - When you click on the YouTube tab:
     - Firefox code needs to access the memory where the tab’s data (HTML,
       JavaScript, video buffer, etc.) is stored.
     - The virtual address of that tab’s data is generated by Firefox (say,
       VA = Page 10 + Offset 512).
     - The MMU (Memory Management Unit) + Page Table do the translation:
       - Page 10 → Frame 247
       - Offset 512 → just added to frame base.
       - Physical Address = Frame 247 + Offset 512.
       - CPU fetches the actual data from that physical location in RAM.

4. What if Page 10 isn’t in RAM?
   - If the YouTube tab hasn’t been used for a while, its memory pages may have been
     swapped out to disk (paging file).
   - When you switch to the tab, a page fault occurs:
     - The OS notices Page 10 isn’t in RAM.
     - It loads Page 10 back into a free frame in RAM (maybe Frame 500 this time).
     - Updates the page table: “Page 10 → Frame 500”.
   - Firefox continues as if nothing happened — it doesn’t know about the swap.

## Advantages of Paging

- No External Fragmentation
  - Unlike variable-sized partitions, paging ensures memory is used efficiently.
  - External fragmentation (gaps between allocated blocks) doesn’t occur.
- Downside: Internal fragmentation is possible (unused space inside a page).
  - Example: if page size = 4KB but process only needs 3.7KB → 0.3KB wasted.
  - Usually not serious if page size is small (e.g., Linux uses 4KB pages).
- Easy Sharing of Memory
  - Each Page Table Entry (PTE) stores the physical frame address.
  - Two processes can share the same physical memory by having their PTEs point
    to the same frame.
  - This allows:
    - Shared libraries (e.g., multiple apps using the same system DLL).
    - Inter-process communication.

## Issues with Paging

- Memory Access Overhead
  - Each virtual memory access may need two physical accesses:
    - One to read the page table entry.
    - One to actually fetch the data.
  - This makes memory access slower.
  - Solution: TLB (Translation Lookaside Buffer) caches recent page table entries
    to speed this up.
- Large Page Tables
  - For large processes with big address spaces, page tables themselves can be huge.
  - Example:
    - A 32-bit address space with 4KB pages → needs 1 million entries per process.
    - Storing all those tables consumes significant memory.
  - Solution: hierarchical page tables, inverted page tables, or multi-level paging.

# Virtual Memory

A memory scheme where secondary storage (disk) is used as if it were part of main
memory (RAM).

Gives the illusion of having more RAM than is physically available.

Supports:

- Very large processes (bigger than RAM).
- Many processes running together.

Only a small part of each process (its active pages) needs to be in RAM at a time.

## Execution of a Process

- Resident Set = the portion of a process actually loaded in RAM.
- If a program accesses a page that is not in memory → Page Fault.
  - CPU triggers an interrupt.
  - OS pauses the process, loads the needed page from disk into RAM.
- If RAM is full → OS swaps out an old page back to disk (replacement).

## Swap Space

Special reserved area on disk to store pages temporarily.

- Linux example: swap space is divided into 4KB slots.

A swap map keeps track of which slots are free/used.

- The page’s swap location can be stored in the Page Table Entry (PTE).

## Locality Principle

When programs run, their memory accesses are not random. Instead, they tend to
follow patterns where the same small areas of memory are used repeatedly or in
sequence. This predictable behavior is called the principle of locality.

Why this works efficiently:

- Temporal locality → if you used it recently, you’ll probably use it again soon.
- Spatial locality → if you use one address, nearby addresses will likely be used next.

So, loading a whole page at once is usually efficient.

## Support Needed

Hardware support (MMU – Memory Management Unit).

OS support (page replacement policies, swapping, managing page tables).

## Issues with Paging

- Big page tables: huge processes mean huge tables.
- Extra memory accesses: translating virtual → physical often takes two accesses:
  - Read page table.
  - Fetch the data.
- Solution: Multi-Level Page Tables
- Page tables themselves are broken into pages.

- Example:
  - Virtual address split into Root Index + Page Table Index + Offset.
  - Only active parts of the page table are kept in RAM.
- Analogy:
  - Instead of one massive phone book, you break it into volumes (A–Z).
    You only pull out the volume you need.

## Virtual Address Format

Example: 32-bit virtual address + 4KB pages

- Offset = 12 bits
- Middle index = 10 bits
- Root index = 10 bits

Example: 16KB space + 64-byte pages

- VA = 14 bits
- Offset = 6 bits
- Page number = 8 bits
- Two-level → split into 3 bits (root) + 5 bits (page index) + 6 bits (offset).

## Inverted Page Table

Instead of one page table per process, use one global table.

- Entries = number of physical frames.
- Each entry stores:
  - Page number
  - Process ID
  - Control bits (valid, referenced…)
  - Chain pointer (for hash collisions)
- Saves space, but slower lookup.

Analogy: Instead of each student keeping their own index card, the library keeps one
giant index by shelf.

## Translation Lookaside Buffer (TLB)

- A cache for page table entries.
- Stores the most recently used translations (Page → Frame).
- Avoids going to RAM every time for page table lookup.
- Associative mapping: TLB is checked in parallel for a match.

Analogy: Sticky notes on your desk for the pages you’re using most often — no need to
open the index card every time.

## Core i7 Example

Uses multiple cache + TLB layers:

- L1 cache, L2 cache, L3 cache.
- L1 + L2 TLBs.

Shows how modern CPUs optimize virtual memory translation.
